{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Hearth-Stone-Python-Simulator/blob/main/welcome_to_colab__3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-_po4xqrjrV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Embedding, Bidirectional, Attention, Concatenate, Masking, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import gc\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5T7DysGQjIb",
        "outputId": "853e4843-4b2e-4a70-9a03-20288bbf22aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNk74K2xrlcM"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"/content/drive/MyDrive/MTGdata/AtomicCards.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDWz2gErzRI"
      },
      "outputs": [],
      "source": [
        "def replace_card_name(index, text):\n",
        "  name_parts = index.split(',')  # Split by comma\n",
        "  possible_matches = [index]\n",
        "\n",
        "  # Add individual name parts if comma exists\n",
        "  if len(name_parts) > 1:\n",
        "    possible_matches.extend([part.strip() for part in name_parts])\n",
        "\n",
        "  # Add permutation for names with multiple words before comma\n",
        "  first_part = name_parts[0].strip()  # Get the part before comma\n",
        "  first_part_words = first_part.split()  # Split into words\n",
        "  if len(first_part_words) > 1:\n",
        "      possible_matches.append(first_part_words[0]) # Add the first word as a match\n",
        "\n",
        "  # Replace occurrences of possible matches in the text, using word boundaries\n",
        "  for name in possible_matches:\n",
        "    text = re.sub(r'\\b' + re.escape(name) + r'\\b', 'this', text) # Use re.escape and word boundaries\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deo6Hcei0RWH"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    text = re.sub(r'\\/', ' ', text)\n",
        "    text = re.sub(r'\\{|\\}', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\+\\-]', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcV0DPJ3sEgG"
      },
      "outputs": [],
      "source": [
        "text_list = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    # Process the indices you're interested in: 0, 1, and 3\n",
        "    for data_index in [0, 1, 3, 4, 5, 6]:\n",
        "        try:\n",
        "            text = row['data'][data_index]['text']\n",
        "            text = replace_card_name(index, text)\n",
        "            text = clean_text(text)\n",
        "            text_list.append('sos ' + text + ' eos')\n",
        "        except:\n",
        "            pass  # Silently handle the exception\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-P3eGNS9bW1",
        "outputId": "e4a7c248-b96b-469a-b1e1-e3f2260de09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Assuming your list of strings is named 'text_list'\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_list)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "# Convert text to sequences\n",
        "input_sequences = tokenizer.texts_to_sequences(text_list)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "encoder_input_data = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Create a mask with pre-padding\n",
        "mask = np.zeros_like(encoder_input_data)\n",
        "for i, seq in enumerate(encoder_input_data):\n",
        "    mask[i, :len(seq)] = 1  # Mask the actual content, leaving padding tokens unmasked\n",
        "\n",
        "# Create decoder input data (shifted by one timestep) and decoder target data\n",
        "decoder_input_data = np.zeros_like(encoder_input_data)\n",
        "decoder_target_data = np.zeros_like(encoder_input_data)\n",
        "\n",
        "for i, seq in enumerate(encoder_input_data):\n",
        "  decoder_input_data[i, 1:] = seq[:-1]  # Shifted by one timestep\n",
        "  decoder_target_data[i, :-1] = seq[1:]  # Target is the original sequence without the start token\n",
        "\n",
        "embedding = Embedding(total_words, 256, input_length=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class zerosLayer(tf.keras.Layer):\n",
        "    def call(self, x):\n",
        "        return tf.zeros_like(x)"
      ],
      "metadata": {
        "id": "lTAwnV1B35hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder model\n",
        "encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = embedding(encoder_inputs)\n",
        "encoder_lstm1 = LSTM(256, return_sequences=True)\n",
        "encoder_lstm2 = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "lstm1_output = encoder_lstm1(encoder_embedding)\n",
        "encoder_outputs, lstm_h, lstm_c = encoder_lstm2(lstm1_output)\n",
        "\n",
        "state_h = zerosLayer()(lstm_h)\n",
        "state_c = lstm_c\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "# Define encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "metadata": {
        "id": "wy8wZHX_t_ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder model\n",
        "decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "decoder_inputs = Dropout(0.2)(decoder_inputs)\n",
        "decoder_embedding = embedding(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "decoder_dense = Dense(total_words, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define decoder model\n",
        "decoder_model = Model([decoder_inputs] + encoder_states, [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "kJCdzWf3uCyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect the encoder and decoder\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=64, epochs=100, validation_split=0.2, sample_weight=mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZy5dQ_tuFGr",
        "outputId": "ae1c1aac-1b70-4106-95eb-b20cede7e955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 76ms/step - accuracy: 0.8937 - loss: 1.2056 - val_accuracy: 0.9132 - val_loss: 0.4739\n",
            "Epoch 2/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 76ms/step - accuracy: 0.9188 - loss: 0.4387 - val_accuracy: 0.9255 - val_loss: 0.3957\n",
            "Epoch 3/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 77ms/step - accuracy: 0.9293 - loss: 0.3678 - val_accuracy: 0.9317 - val_loss: 0.3536\n",
            "Epoch 4/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9351 - loss: 0.3266 - val_accuracy: 0.9366 - val_loss: 0.3192\n",
            "Epoch 5/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9390 - loss: 0.2976 - val_accuracy: 0.9389 - val_loss: 0.3012\n",
            "Epoch 6/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9419 - loss: 0.2772 - val_accuracy: 0.9406 - val_loss: 0.2913\n",
            "Epoch 7/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9438 - loss: 0.2642 - val_accuracy: 0.9428 - val_loss: 0.2767\n",
            "Epoch 8/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9460 - loss: 0.2512 - val_accuracy: 0.9455 - val_loss: 0.2626\n",
            "Epoch 9/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9486 - loss: 0.2361 - val_accuracy: 0.9462 - val_loss: 0.2563\n",
            "Epoch 10/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9511 - loss: 0.2227 - val_accuracy: 0.9488 - val_loss: 0.2442\n",
            "Epoch 11/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9525 - loss: 0.2148 - val_accuracy: 0.9511 - val_loss: 0.2336\n",
            "Epoch 12/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9552 - loss: 0.2010 - val_accuracy: 0.9522 - val_loss: 0.2280\n",
            "Epoch 13/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9564 - loss: 0.1940 - val_accuracy: 0.9531 - val_loss: 0.2232\n",
            "Epoch 14/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9583 - loss: 0.1851 - val_accuracy: 0.9536 - val_loss: 0.2209\n",
            "Epoch 15/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9597 - loss: 0.1769 - val_accuracy: 0.9557 - val_loss: 0.2121\n",
            "Epoch 16/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9608 - loss: 0.1706 - val_accuracy: 0.9562 - val_loss: 0.2082\n",
            "Epoch 17/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9618 - loss: 0.1649 - val_accuracy: 0.9576 - val_loss: 0.2023\n",
            "Epoch 18/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9629 - loss: 0.1598 - val_accuracy: 0.9581 - val_loss: 0.2001\n",
            "Epoch 19/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9636 - loss: 0.1555 - val_accuracy: 0.9591 - val_loss: 0.1961\n",
            "Epoch 20/100\n",
            "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - accuracy: 0.9648 - loss: 0.1494 - val_accuracy: 0.9596 - val_loss: 0.1938\n",
            "Epoch 21/100\n",
            "\u001b[1m305/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.9658 - loss: 0.1442"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_encoder_decoder(encoder_model, decoder_model, tokenizer, text_lst, sample_size=5, max_sequence_length=max_sequence_length):\n",
        "  \"\"\"Tests the encoder-decoder network on a small sample of text.\n",
        "\n",
        "  Args:\n",
        "    encoder_model: The trained encoder model.\n",
        "    decoder_model: The trained decoder model.\n",
        "    tokenizer: The tokenizer used to convert text to sequences.\n",
        "    text_lst: The list of text strings.\n",
        "    sample_size: The number of samples to test.\n",
        "    max_sequence_length: The maximum sequence length used during training.\n",
        "\n",
        "  Returns:\n",
        "    None. Prints the original and predicted text for each sample.\n",
        "  \"\"\"\n",
        "  # Get a random sample from text_lst\n",
        "  sample_indices = random.sample(range(len(text_lst)), sample_size)\n",
        "  sample_texts = [text_lst[i] for i in sample_indices]\n",
        "\n",
        "  for text in sample_texts:\n",
        "    # Encode the input text\n",
        "    input_seq = tokenizer.texts_to_sequences([text])[0]  # Get the sequence\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_sequence_length, padding='post')  # Pad\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate the output sequence\n",
        "    target_seq = np.zeros((1, 1))  # Start with a single token (e.g., start token)\n",
        "    target_seq[0, 0] = tokenizer.word_index['sos']  # Assuming '<SOS> ' is your start token\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    stop_condition = False\n",
        "\n",
        "    while not stop_condition:\n",
        "      output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])  # Sample the next token\n",
        "\n",
        "      sampled_word = tokenizer.index_word[sampled_token_index] if sampled_token_index != 0 else 'eos'  # Get the word\n",
        "      decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "      if sampled_word == 'eos' or len(decoded_sentence.split()) > max_sequence_length:\n",
        "        stop_condition = True\n",
        "\n",
        "      target_seq = np.zeros((1, 1))\n",
        "      target_seq[0, 0] = sampled_token_index\n",
        "      states_value = [h, c]\n",
        "\n",
        "    print('Original:', text)\n",
        "    print('Predicted:', decoded_sentence)\n",
        "    print('---')\n",
        "\n",
        "test_encoder_decoder(encoder_model, decoder_model, tokenizer, text_list, sample_size=3)  # Test with 3 samples\n"
      ],
      "metadata": {
        "id": "snZ1o3un4QP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}