{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tSX_ogfdCk-Kk7D8faxXfkcBp6iPuRnq",
      "authorship_tag": "ABX9TyP9nnv0d4MbFL3sAqG95UlU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Hearth-Stone-Python-Simulator/blob/main/card%20predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "# GPU configuration (already provided)\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        # Enable memory growth for all GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "qRkkNa0SuzDh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CardEncodingLookup layer (already provided)\n",
        "class CardEncodingLookup(tf.keras.layers.Layer):\n",
        "    def __init__(self, encoding_path, **kwargs):\n",
        "        super(CardEncodingLookup, self).__init__(**kwargs)\n",
        "        self.encoding_path = encoding_path\n",
        "        # Load encodings during initialization\n",
        "        with open(encoding_path, \"rb\") as f:\n",
        "            self.encodings = pickle.load(f)\n",
        "        # Get list of card names\n",
        "        self.card_names = list(self.encodings.keys())\n",
        "\n",
        "        # Create a lookup table for card indices\n",
        "        # CHANGE: Use 0 as default_value instead of -1\n",
        "        self.table = tf.lookup.StaticHashTable(\n",
        "            tf.lookup.KeyValueTensorInitializer(\n",
        "                tf.constant(self.card_names, dtype=tf.string),\n",
        "                tf.range(1, len(self.card_names) + 1, dtype=tf.int64),  # Start from 1\n",
        "            ),\n",
        "            default_value=0,  # Use 0 for unknown cards\n",
        "        )\n",
        "\n",
        "        # Add a special \"unknown\" card embedding at index 0\n",
        "        unknown_card = {\n",
        "            \"colorid\": np.zeros_like(self.encodings[self.card_names[0]][\"colorid\"]),\n",
        "            \"text\": np.zeros_like(self.encodings[self.card_names[0]][\"text\"]),\n",
        "            \"types\": np.zeros_like(self.encodings[self.card_names[0]][\"types\"]),\n",
        "            \"mana\": np.zeros_like(self.encodings[self.card_names[0]][\"mana\"]),\n",
        "            \"stats\": np.zeros_like(self.encodings[self.card_names[0]][\"stats\"]),\n",
        "        }\n",
        "\n",
        "        # Convert encodings to tensors and store them with the unknown card at index 0\n",
        "        self.colorid_tensors = tf.constant(\n",
        "            [unknown_card[\"colorid\"]]\n",
        "            + [self.encodings[name][\"colorid\"] for name in self.card_names]\n",
        "        )\n",
        "        self.text_tensors = tf.constant(\n",
        "            [unknown_card[\"text\"]]\n",
        "            + [self.encodings[name][\"text\"] for name in self.card_names]\n",
        "        )\n",
        "        self.types_tensors = tf.constant(\n",
        "            [unknown_card[\"types\"]]\n",
        "            + [self.encodings[name][\"types\"] for name in self.card_names]\n",
        "        )\n",
        "        self.mana_tensors = tf.constant(\n",
        "            [unknown_card[\"mana\"]]\n",
        "            + [self.encodings[name][\"mana\"] for name in self.card_names]\n",
        "        )\n",
        "        self.stats_tensors = tf.constant(\n",
        "            [unknown_card[\"stats\"]]\n",
        "            + [self.encodings[name][\"stats\"] for name in self.card_names]\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Look up the indices for the card names\n",
        "        indices = self.table.lookup(inputs)\n",
        "\n",
        "        # Gather the encodings for the indices\n",
        "        colorid = tf.gather(self.colorid_tensors, indices)\n",
        "        text = tf.gather(self.text_tensors, indices)\n",
        "        types = tf.gather(self.types_tensors, indices)\n",
        "        mana = tf.gather(self.mana_tensors, indices)\n",
        "        stats = tf.gather(self.stats_tensors, indices)\n",
        "\n",
        "        return {\n",
        "            \"colorid\": colorid,\n",
        "            \"text\": text,\n",
        "            \"types\": types,\n",
        "            \"mana\": mana,\n",
        "            \"stats\": stats,\n",
        "        }\n",
        "\n",
        "# Create the lookup layer (already defined in the provided code)\n",
        "card_lookup = CardEncodingLookup(encoding_path=\"data/card_encodings.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5LZHodyBvWTx",
        "outputId": "6c9231cd-d82f-4b40-fcf5-b1b8f1a7e9ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/card_encodings.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b490f8d7f0c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Create the lookup layer (already defined in the provided code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mcard_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCardEncodingLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/card_encodings.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-b490f8d7f0c4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoding_path, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Load encodings during initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Get list of card names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/card_encodings.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a model for synergy prediction\n",
        "class SynergyModel(tf.keras.Model):\n",
        "    def __init__(self, card_lookup, **kwargs):\n",
        "        super(SynergyModel, self).__init__(**kwargs)\n",
        "        self.card_lookup = card_lookup\n",
        "\n",
        "        # Feature processing layers\n",
        "        self.colorid_dense = tf.keras.layers.Dense(32, activation=\"leaky_relu\")\n",
        "        self.text_lstm = Bidirectional(\n",
        "            tf.keras.layers.LSTM(\n",
        "                64,\n",
        "                return_sequences=True,\n",
        "            )\n",
        "        )\n",
        "        self.text_lstm2 = Bidirectional(tf.keras.layers.LSTM(64))\n",
        "        self.types_dense = tf.keras.layers.Dense(32, activation=\"leaky_relu\")\n",
        "        self.mana_dense = tf.keras.layers.Dense(32, activation=\"leaky_relu\")\n",
        "        self.stats_dense = tf.keras.layers.Dense(32, activation=\"leaky_relu\")\n",
        "\n",
        "        # Combination layers\n",
        "        self.combine_dense1 = tf.keras.layers.Dense(256, activation=\"leaky_relu\")\n",
        "        self.combine_dense2 = tf.keras.layers.Dense(32, activation=\"leaky_relu\")\n",
        "        self.output_layer = tf.keras.layers.Dense(1)  # Single output for synergy score\n",
        "\n",
        "    def process_card(self, card_encodings):\n",
        "        # Process each feature type\n",
        "        colorid_features = self.colorid_dense(card_encodings[\"colorid\"])\n",
        "\n",
        "        # Extract the text tensor for LSTM - this is the key fix\n",
        "        text_tensor = card_encodings[\"text\"]\n",
        "        # Ensure it has the right shape for LSTM (batch_size, timesteps, features)\n",
        "        text_features = self.text_lstm(text_tensor)\n",
        "        text_features = self.text_lstm2(text_features)\n",
        "\n",
        "        types_features = self.types_dense(card_encodings[\"types\"])\n",
        "        mana_features = self.mana_dense(card_encodings[\"mana\"])\n",
        "        stats_features = self.stats_dense(card_encodings[\"stats\"])\n",
        "\n",
        "        # Concatenate all features\n",
        "        return tf.concat(\n",
        "            [\n",
        "                colorid_features,\n",
        "                text_features,\n",
        "                types_features,\n",
        "                mana_features,\n",
        "                stats_features,\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        card1_name, card2_name = inputs[\"card_1\"], inputs[\"card_2\"]\n",
        "\n",
        "        # Get encodings for both cards\n",
        "        card1_encodings = self.card_lookup(card1_name)\n",
        "        card2_encodings = self.card_lookup(card2_name)\n",
        "\n",
        "        # Process each card\n",
        "        card1_features = self.process_card(card1_encodings)\n",
        "        card2_features = self.process_card(card2_encodings)\n",
        "\n",
        "        # Combine features from both cards\n",
        "        combined_features = tf.concat([card1_features, card2_features], axis=1)\n",
        "\n",
        "        # Final prediction layers\n",
        "        x = self.combine_dense1(combined_features)\n",
        "        x = self.combine_dense2(x)\n",
        "        synergy = self.output_layer(x)\n",
        "\n",
        "        return synergy"
      ],
      "metadata": {
        "id": "TfGe_0ICvZrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Function to load and preprocess the CSV data\n",
        "def load_synergy_data(csv_path, test_size=0.05, random_state=42):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Get indices for train/test split\n",
        "    indices = np.arange(len(df))\n",
        "    indices_train, indices_test = train_test_split(\n",
        "        indices, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split the dataframe\n",
        "    df_train = df.iloc[indices_train]\n",
        "    df_test = df.iloc[indices_test]\n",
        "\n",
        "    # Create input dictionaries and output arrays\n",
        "    X_train = {\n",
        "        \"card_1\": df_train[\"main_card\"].values,\n",
        "        \"card_2\": df_train[\"combo_card\"].values,\n",
        "    }\n",
        "    X_test = {\n",
        "        \"card_1\": df_test[\"main_card\"].values,\n",
        "        \"card_2\": df_test[\"combo_card\"].values,\n",
        "    }\n",
        "\n",
        "    df_train[\"synergies_score\"] = df_train[\"synergies_score\"] * 0.05\n",
        "    df_test[\"synergies_score\"] = df_test[\"synergies_score\"] * 0.05\n",
        "\n",
        "    y_train = df_train[\"synergies_score\"].values\n",
        "    y_test = df_test[\"synergies_score\"].values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Load and preprocess data\n",
        "# Assuming you have a CSV file with columns: card_1, card_2, synergy\n",
        "X_train, X_test, y_train, y_test = load_synergy_data(\"/content/drive/MyDrive/synergy_dataset.csv\")"
      ],
      "metadata": {
        "id": "0u9ER9cuv1Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_erjrwPtutx9"
      },
      "outputs": [],
      "source": [
        "# 3. Function to train the model\n",
        "def train_synergy_model(\n",
        "    model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32\n",
        "):\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"],\n",
        "    )\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(\n",
        "        batch_size\n",
        "    )\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_loss\", factor=0.5, patience=3\n",
        "            ),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# 4. Function to evaluate and visualize results\n",
        "def evaluate_model(model, X_test, y_test, history=None):\n",
        "    # Create test dataset\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = model.evaluate(test_dataset)\n",
        "    print(f\"Test Loss: {results[0]:.4f}\")\n",
        "    print(f\"Test MAE: {results[1]:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Plot predictions vs actual\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
        "    plt.xlabel(\"Actual Synergy\")\n",
        "    plt.ylabel(\"Predicted Synergy\")\n",
        "    plt.title(\"Synergy Prediction: Actual vs Predicted\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training history if available\n",
        "    if history:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history[\"loss\"])\n",
        "        plt.plot(history.history[\"val_loss\"])\n",
        "        plt.title(\"Model Loss\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history[\"mae\"])\n",
        "        plt.plot(history.history[\"val_mae\"])\n",
        "        plt.title(\"Model MAE\")\n",
        "        plt.ylabel(\"MAE\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# 5. Function to predict synergy for new card pairs\n",
        "def predict_synergy(model, card1_name, card2_name):\n",
        "    # Create input dictionary\n",
        "    input_data = {\"card_1\": np.array([card1_name]), \"card_2\": np.array([card2_name])}\n",
        "\n",
        "    # Make prediction\n",
        "    synergy = model.predict(input_data)[0][0]\n",
        "\n",
        "    return synergy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the synergy model\n",
        "synergy_model = SynergyModel(card_lookup)\n",
        "\n",
        "synergy_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = train_synergy_model(\n",
        "    synergy_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,  # Using test set as validation for simplicity\n",
        "    epochs=10,\n",
        "    batch_size=1024,\n",
        ")\n",
        "\n",
        "synergy_model.save_weights(\"synergy_model.weights.h5\")"
      ],
      "metadata": {
        "id": "i5mnWeOSu1c3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}