{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aetev/Hearth-Stone-Python-Simulator/blob/main/text_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx4GYeOpd3Ka",
        "outputId": "d602f75f-4f47-4e58-b8a8-2a738076ca3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-10 20:48:08.696512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741657688.715793   11270 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741657688.722005   11270 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 20:48:08.740795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, TextVectorization\n",
        "# Assuming AtomicCards.json is in your Google Drive, adjust the path if necessary\n",
        "file_path = 'Data/AtomicCards.json'\n",
        "\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  data = json.load(f)['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9rUDffCq5X2"
      },
      "outputs": [],
      "source": [
        "def filter_legal_cards(data):\n",
        "    legal_commander_data = {}\n",
        "    for card_name, card_data in data.items():\n",
        "        try:\n",
        "            if card_data[0]['legalities']['commander'] == 'Legal':\n",
        "                legal_commander_data[card_name] = card_data[0] # Store only the first element of card_data\n",
        "        except (KeyError, IndexError):\n",
        "            pass  # Or print a message, log it, etc.\n",
        "    return legal_commander_data\n",
        "\n",
        "filtered_data = filter_legal_cards(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXajsrp-VOCl",
        "outputId": "14b167ab-329d-4658-d84b-16b8578f176f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:49: SyntaxWarning: invalid escape sequence '\\/'\n",
            "<>:51: SyntaxWarning: invalid escape sequence '\\{'\n",
            "<>:52: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:53: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:54: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:49: SyntaxWarning: invalid escape sequence '\\/'\n",
            "<>:51: SyntaxWarning: invalid escape sequence '\\{'\n",
            "<>:52: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:53: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:54: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipykernel_11270/1707253964.py:49: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  cleaned_text = tf.strings.regex_replace(cleaned_text, '[\\/\\.]', ' ')\n",
            "/tmp/ipykernel_11270/1707253964.py:51: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  cleaned_text = tf.strings.regex_replace(cleaned_text, '\\{|\\}', ' ')\n",
            "/tmp/ipykernel_11270/1707253964.py:52: SyntaxWarning: invalid escape sequence '\\('\n",
            "  cleaned_text = tf.strings.regex_replace(cleaned_text, '\\([^)]*\\)', '')\n",
            "/tmp/ipykernel_11270/1707253964.py:53: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  cleaned_text = tf.strings.regex_replace(cleaned_text, '[^\\w\\s+/\\-]', '')\n",
            "/tmp/ipykernel_11270/1707253964.py:54: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  cleaned_text = tf.strings.regex_replace(cleaned_text, '\\s+', ' ')\n"
          ]
        }
      ],
      "source": [
        "class TextCleaningLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, card_dataset, output_sequence_length=150, embedding_dim=100, max_tokens=19756, **kwargs):\n",
        "        super(TextCleaningLayer, self).__init__(**kwargs)\n",
        "        # Store configuration parameters\n",
        "        self.output_sequence_length = output_sequence_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "        # Initialize vectorization layer\n",
        "        self.vectorizer = tf.keras.layers.TextVectorization(\n",
        "            standardize=self._clean_text,\n",
        "            output_mode='int',\n",
        "            output_sequence_length=output_sequence_length,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "\n",
        "        # Extract card texts more efficiently using list comprehension\n",
        "        card_texts = [card_name for card_name in card_dataset.keys()]\n",
        "        card_texts.extend([\n",
        "            card_info['text'] for card_info in card_dataset.values()\n",
        "            if 'text' in card_info and card_info['text']\n",
        "        ])\n",
        "\n",
        "        # Create an optimized TensorFlow dataset\n",
        "        card_texts_ds = tf.data.Dataset.from_tensor_slices(card_texts)\n",
        "\n",
        "        # Apply performance optimizations\n",
        "        card_texts_ds = card_texts_ds.batch(512)  # Use larger batches\n",
        "        card_texts_ds = card_texts_ds.prefetch(tf.data.AUTOTUNE)  # Prefetch next batch\n",
        "\n",
        "        # Adapt the vectorizer with the dataset\n",
        "        self.vectorizer.adapt(card_texts_ds)\n",
        "\n",
        "        # Initialize embedding layer\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(\n",
        "            input_dim=max_tokens,\n",
        "            output_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Clean text by applying various regex transformations\"\"\"\n",
        "        # Convert input to string tensor if it's not already\n",
        "        if not isinstance(text, tf.Tensor):\n",
        "            text = tf.convert_to_tensor(text, dtype=tf.string)\n",
        "\n",
        "        # Apply cleaning operations\n",
        "        cleaned_text = tf.strings.lower(text)\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '\\n', ' ')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '[\\/\\.]', ' ')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '[\\\"â€”\"]', ' ')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '\\{|\\}', ' ')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '\\([^)]*\\)', '')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '[^\\w\\s+/\\-]', '')\n",
        "        cleaned_text = tf.strings.regex_replace(cleaned_text, '\\s+', ' ')\n",
        "        return cleaned_text\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Process inputs through the layer pipeline\"\"\"\n",
        "        # Vectorize the text (cleaning is handled by the vectorizer)\n",
        "        vectorized_text = self.vectorizer(inputs)\n",
        "        # Pass through embedding layer\n",
        "        embedded_text = self.embedding_layer(vectorized_text)\n",
        "        return embedded_text\n",
        "\n",
        "    def get_cleaned_text(self, inputs):\n",
        "        \"\"\"Utility method to get only the cleaned text without vectorization/embedding\"\"\"\n",
        "        return self._clean_text(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SiKHaZyU0ds"
      },
      "outputs": [],
      "source": [
        "card = filtered_data['Aatchik, Emerald Radian']\n",
        "cleaned_text_layer = TextCleaningLayer(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck0PB6l15VFJ",
        "outputId": "1e508e04-9902-454b-cfec-06da2cf463af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19756\n"
          ]
        }
      ],
      "source": [
        "print(len(cleaned_text_layer.vectorizer.get_vocabulary()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}